{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Машинное обучение, ВМК МГУ\n",
    "\n",
    "## Практическое задание 2\n",
    "\n",
    "### Общая информация\n",
    "Дата выдачи: 9 октября 2019\n",
    "\n",
    "Максимальная оценка: 10 баллов + 1 бонусный балл\n",
    "\n",
    "Мягкий дедлайн: 23:59MSK 23 октября (за каждый день просрочки снимается 1 балл)\n",
    "\n",
    "Жесткий дедлайн: 23:59MSK 30 октября."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "В этом задании вы:\n",
    "- Познакомитесь с методом решения задачи регрессии на основе метода ближайших соседей.\n",
    "- Реализуете алгоритм kNN для задачи регрессии.\n",
    "- Изучите методы работы с категориальными и текстовыми переменными.\n",
    "\n",
    "\n",
    "### Оценивание и штрафы\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов.\n",
    "\n",
    "Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "\n",
    "\n",
    "### Формат сдачи\n",
    "Для сдачи задания переименуйте получившийся файл *.ipynb в соответствии со следующим форматом: homework-practice-02-Username.ipynb, где Username — ваша фамилия и имя на латинице именно в таком порядке (например, homework-practice-02-ivanov.ipynb).\n",
    "\n",
    "Далее отправьте этот файл на anytask в соответсвующий раздел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все эксперименты в этой лабораторной работе предлагается проводить на данных соревнования New York City Airbnb Open Data: https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data#AB_NYC_2019.csv\n",
    "\n",
    "В данной задаче предлагается предсказать цену на съем квартиры в зависимости от её параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_name</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>room_type</th>\n",
       "      <th>price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>last_review</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2539</td>\n",
       "      <td>Clean &amp; quiet apt home by the park</td>\n",
       "      <td>2787</td>\n",
       "      <td>John</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Kensington</td>\n",
       "      <td>40.64749</td>\n",
       "      <td>-73.97237</td>\n",
       "      <td>Private room</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2018-10-19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>6</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2595</td>\n",
       "      <td>Skylit Midtown Castle</td>\n",
       "      <td>2845</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Midtown</td>\n",
       "      <td>40.75362</td>\n",
       "      <td>-73.98377</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2</td>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3647</td>\n",
       "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
       "      <td>4632</td>\n",
       "      <td>Elisabeth</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Harlem</td>\n",
       "      <td>40.80902</td>\n",
       "      <td>-73.94190</td>\n",
       "      <td>Private room</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3831</td>\n",
       "      <td>Cozy Entire Floor of Brownstone</td>\n",
       "      <td>4869</td>\n",
       "      <td>LisaRoxanne</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Clinton Hill</td>\n",
       "      <td>40.68514</td>\n",
       "      <td>-73.95976</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>270</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>4.64</td>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5022</td>\n",
       "      <td>Entire Apt: Spacious Studio/Loft by central park</td>\n",
       "      <td>7192</td>\n",
       "      <td>Laura</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>East Harlem</td>\n",
       "      <td>40.79851</td>\n",
       "      <td>-73.94399</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              name  host_id  \\\n",
       "0  2539                Clean & quiet apt home by the park     2787   \n",
       "1  2595                             Skylit Midtown Castle     2845   \n",
       "2  3647               THE VILLAGE OF HARLEM....NEW YORK !     4632   \n",
       "3  3831                   Cozy Entire Floor of Brownstone     4869   \n",
       "4  5022  Entire Apt: Spacious Studio/Loft by central park     7192   \n",
       "\n",
       "     host_name neighbourhood_group neighbourhood  latitude  longitude  \\\n",
       "0         John            Brooklyn    Kensington  40.64749  -73.97237   \n",
       "1     Jennifer           Manhattan       Midtown  40.75362  -73.98377   \n",
       "2    Elisabeth           Manhattan        Harlem  40.80902  -73.94190   \n",
       "3  LisaRoxanne            Brooklyn  Clinton Hill  40.68514  -73.95976   \n",
       "4        Laura           Manhattan   East Harlem  40.79851  -73.94399   \n",
       "\n",
       "         room_type  price  minimum_nights  number_of_reviews last_review  \\\n",
       "0     Private room    149               1                  9  2018-10-19   \n",
       "1  Entire home/apt    225               1                 45  2019-05-21   \n",
       "2     Private room    150               3                  0         NaN   \n",
       "3  Entire home/apt     89               1                270  2019-07-05   \n",
       "4  Entire home/apt     80              10                  9  2018-11-19   \n",
       "\n",
       "   reviews_per_month  calculated_host_listings_count  availability_365  \n",
       "0               0.21                               6               365  \n",
       "1               0.38                               2               355  \n",
       "2                NaN                               1               365  \n",
       "3               4.64                               1               194  \n",
       "4               0.10                               1                 0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('AB_NYC_2019.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48895, 16)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 48895\n",
      "name 47906\n",
      "host_id 37457\n",
      "host_name 11453\n",
      "neighbourhood_group 5\n",
      "neighbourhood 221\n",
      "latitude 19048\n",
      "longitude 14718\n",
      "room_type 3\n",
      "price 674\n",
      "minimum_nights 109\n",
      "number_of_reviews 394\n",
      "last_review 1765\n",
      "reviews_per_month 938\n",
      "calculated_host_listings_count 47\n",
      "availability_365 366\n"
     ]
    }
   ],
   "source": [
    "# число значений у признаков\n",
    "for col_name in data.columns:\n",
    "    print(col_name, len(data[col_name].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                    0\n",
       "name                                 16\n",
       "host_id                               0\n",
       "host_name                            21\n",
       "neighbourhood_group                   0\n",
       "neighbourhood                         0\n",
       "latitude                              0\n",
       "longitude                             0\n",
       "room_type                             0\n",
       "price                                 0\n",
       "minimum_nights                        0\n",
       "number_of_reviews                     0\n",
       "last_review                       10052\n",
       "reviews_per_month                 10052\n",
       "calculated_host_listings_count        0\n",
       "availability_365                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видите, в данных есть пропуски. Не забудьте обработать их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь (ノ°∀°)ノ⌒･*:.｡. .｡.:*･゜ﾟ･*☆\n",
    "numberCols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numberCols.remove('id')\n",
    "numberCols.remove('price')\n",
    "\n",
    "categoricalCols = ['neighbourhood_group', 'neighbourhood', 'room_type']\n",
    "\n",
    "data[numberCols] = data[numberCols].fillna(0)\n",
    "data[categoricalCols] = data[categoricalCols].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобъем данные на обучение и контроль."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['price']), data[['price']],\n",
    "                                                    test_size=0.3, random_state=241)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1: Алгоритм kNN в задаче регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1.1 (1.5 балла) </b>\n",
    "Реализуйте класс `KNNRegressor`, который используя метод k ближайших соседей решает задачу регрессии. Для решение данной задачи, необходимо найти $N_k$ - k соседей, и после использовать значения их целевых переменных для предсказания:\n",
    "\\begin{align}\n",
    "y = \\frac{1}{k}\\sum_{n \\in N_k}w_n y_n,\n",
    "\\end{align}\n",
    "\n",
    "где $w_n$ - вес каждого соседа. \n",
    "\n",
    "При этом `KNNRegressor` может работать в 2 режимах:\n",
    " - $uniform$ - ближайшие соседи учитываются с одинаковыми весами.\n",
    " - $distance$ - вес ближайших соседей зависит от расстояния\n",
    " \n",
    "Сигнатуру методов при желании можно менять."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Callable, Iterable, Optional\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "class KNNRegressor:\n",
    "    def __init__(self, n_neighbors: int, metric: Union[str, Callable], mode: str = 'uniform'):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            n_neighbors: number of neighbors\n",
    "            metric: metric to use for distance computation\n",
    "            mode: 'uniform' or 'distance'\n",
    "            'uniform' - all points in each neighborhood are weighted equally\n",
    "            'distance' - weight points by the inverse of their distance\n",
    "        \"\"\"\n",
    "#         self.__nn = NearestNeighbors(n_neighbors = n_neighbors, metric = metric)\n",
    "#         self.__nnTest = KNeighborsRegressor(n_neighbors = n_neighbors, metric = metric, weights = mode)\n",
    "        self.__metric = metric\n",
    "        self.__mode = mode\n",
    "        self.__n_neighbors = n_neighbors\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array) -> None:\n",
    "        \"\"\"\n",
    "            X: data\n",
    "            y: labels\n",
    "        \"\"\"\n",
    "        # Ваш код здесь\n",
    "        self.__X = X\n",
    "        self.__y = y\n",
    "#         self.__nn.fit(X, y)\n",
    "#         self.__nnTest.fit(X, y)\n",
    "        \n",
    "    def euclidean_distance(self, X, Y):\n",
    "        d = len(X[0])\n",
    "        res = np.zeros(len(X)*len(Y)).reshape(len(X), len(Y))\n",
    "        for x in range(len(X)):\n",
    "            for y in range(len(Y)):\n",
    "                sum = 0\n",
    "                for f in range(len(Y[y])):\n",
    "                    sum += (X[x][f] - Y[y][f])**2\n",
    "                res[x][y] = np.sqrt(sum)\n",
    "        return res;\n",
    "\n",
    "    def cosine_distance(self, X, Y):\n",
    "        sumyy = (Y**2).sum(1)\n",
    "        sumxx = (X**2).sum(1, keepdims=1)\n",
    "        sumxy = X.dot(Y.T)\n",
    "        return 1 - (sumxy / np.sqrt(sumxx)) / np.sqrt(sumyy)\n",
    "    \n",
    "    def overlap(self, X, Y):\n",
    "        res = []\n",
    "        res = np.zeros(len(X)*len(Y)).reshape(len(X), len(Y))\n",
    "        for x in range(len(X)):\n",
    "            for y in range(len(Y)):\n",
    "                dist = 0\n",
    "                for f in range(len(Y[y])):\n",
    "                    if (X[x][f] != Y[y][f]):\n",
    "                        dist = 1\n",
    "                        break\n",
    "                res[x][y] = dist\n",
    "        return res;\n",
    "\n",
    "    def flattened_overlap(self, X, Y):\n",
    "        # находим частоту признаков\n",
    "        freqs = {}\n",
    "        for col in range(X.shape[1]):\n",
    "            values, freq = np.unique(X[:,col], return_counts=True)\n",
    "            freqs[col] = {values[q]: freq[q] for q in range(len(values))} \n",
    "        \n",
    "        res = np.zeros(len(X)*len(Y)).reshape(len(X), len(Y))\n",
    "        for x in range(len(X)):\n",
    "            for y in range(len(Y)):\n",
    "                dist = 0\n",
    "                p = 0\n",
    "                for c in range(len(Y[y])):\n",
    "                    if (X[x][c] != Y[y][c]):\n",
    "                        dist = 1\n",
    "                        p = 0\n",
    "                        break\n",
    "                    else:\n",
    "                        dp = (freqs[c][X[x][c]] * (freqs[c][X[x][c]] - 1)) / (len(X) * (len(X) - 1 ))\n",
    "                        p = p + dp\n",
    "                \n",
    "                res[x][y] = dist + p\n",
    "        return res;\n",
    "    \n",
    "    def log_overlap(self, X, Y):\n",
    "        \n",
    "        # находим частоту признаков\n",
    "        freqsX = {}\n",
    "        for col in range(X.shape[1]):\n",
    "            values, freq = np.unique(X[:,col], return_counts=True)\n",
    "            freqsX[col] = {values[q]: freq[q] for q in range(len(values))} \n",
    "\n",
    "        freqsY = {}\n",
    "        for col in range(Y.shape[1]):\n",
    "            values, freq = np.unique(Y[:,col], return_counts=True)\n",
    "            freqsY[col] = {values[q]: freq[q] for q in range(len(values))} \n",
    "        \n",
    "        res = np.zeros(len(X)*len(Y)).reshape(len(X), len(Y))\n",
    "        for x in range(len(X)):\n",
    "            for y in range(len(Y)):\n",
    "                dist = 0\n",
    "                for c in range(len(Y[y])):\n",
    "                    if (X[x][c] != Y[y][c]):\n",
    "                        dist = dist + np.log(1 + freqsX[c][X[x][f]]) * np.log(1 + freqsY[c][Y[x][f]])\n",
    "                \n",
    "                res[x][y] = dist\n",
    "        return res;\n",
    "\n",
    "    def find_kneighbors(self, X, return_distance = True):\n",
    "        \n",
    "        if (self.__metric == 'euclidean'):\n",
    "            dsModRes = self.euclidean_distance(X, self.__X)\n",
    "        elif (self.__metric == 'cosine'):\n",
    "            dsModRes = self.cosine_distance(X, self.__X)\n",
    "        elif (self.__metric == 'overlap'):\n",
    "            dsModRes = self.overlap(X, self.__X)\n",
    "        elif (self.__metric == 'flattened_overlap'):\n",
    "            dsModRes = self.flattened_overlap(X, self.__X)\n",
    "        elif (self.__metric == 'log_overlap'):\n",
    "            dsModRes = self.flattened_overlap(X, self.__X)\n",
    "        else:\n",
    "            raise Exception(\"Unknown 'metric' param value\")\n",
    "\n",
    "        res = []\n",
    "        dists = []\n",
    "        for i in range(len(dsModRes)):\n",
    "            tmpDists = np.argsort(dsModRes[i])[:self.__n_neighbors]\n",
    "            res.append(dsModRes[i][tmpDists])\n",
    "            if (return_distance):\n",
    "                dists.append(tmpDists)\n",
    "\n",
    "        if (return_distance):\n",
    "            return (res, dists)\n",
    "        else:\n",
    "            return (res)\n",
    "        \n",
    "    def predict(self, X: np.array, n_neighbors: Optional[int] = None) -> np.array:\n",
    "        \"\"\"\n",
    "            X: data\n",
    "            n_neighbors: number of neighbors\n",
    "        \"\"\"\n",
    "        # Ваш код здесь\n",
    "#         distances, indices = self.__nn.kneighbors(X)\n",
    "#         distances, indices = self.__nnTest.kneighbors(X)\n",
    "        distances, indices = self.find_kneighbors(X)\n",
    "    \n",
    "        if (self.__mode == 'uniform'):\n",
    "            res = self.__y[indices].mean(axis=1)\n",
    "        else: \n",
    "            res = []\n",
    "            for row in range(len(indices)):\n",
    "                predictedVal = 0\n",
    "                weightsSum = 0\n",
    "                for col in range(len(indices[row])):\n",
    "                    w = 1 / (1 + distances[row][col])\n",
    "                    weightsSum = weightsSum + w\n",
    "                    predictedVal = predictedVal + w * self.__y[indices[row][col]]\n",
    "                res.append(predictedVal / weightsSum)\n",
    "            res = np.array(res)\n",
    "#         return self.__nnTest.predict(X)\n",
    "        return res\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 86.66666667 333.33333333 260.         154.66666667 125.33333333\n",
      "  70.         199.66666667 181.33333333 333.33333333  75.66666667\n",
      " 241.66666667 140.          86.66666667  87.66666667 218.66666667\n",
      "  88.66666667  90.33333333 130.         186.66666667 113.\n",
      " 187.66666667 186.         195.          90.          86.66666667\n",
      " 119.66666667  79.33333333  63.         130.         120.\n",
      " 193.33333333 115.66666667  63.         111.         114.33333333\n",
      " 183.33333333  56.33333333 129.33333333 183.33333333 212.33333333\n",
      " 130.          68.33333333 176.66666667 319.66666667 104.66666667\n",
      " 164.33333333 143.33333333  65.66666667 111.66666667  77.66666667\n",
      " 270.         146.66666667 101.66666667 134.66666667 139.33333333\n",
      " 129.          61.33333333  99.33333333 149.66666667  62.\n",
      "  71.66666667  74.66666667  68.33333333  98.66666667  57.33333333\n",
      " 113.33333333  85.66666667  98.         114.33333333 240.\n",
      " 178.33333333  84.66666667 105.66666667  88.33333333 187.66666667\n",
      " 110.66666667 111.33333333 175.          63.          71.\n",
      " 123.33333333  99.         116.66666667 135.66666667 175.33333333\n",
      "  68.33333333  99.33333333  63.         112.66666667  99.33333333\n",
      "  99.33333333 123.         194.66666667 123.          93.33333333\n",
      " 108.33333333 148.33333333 113.33333333 270.         126.\n",
      " 134.66666667  84.          91.66666667 228.         133.33333333\n",
      " 151.33333333  91.66666667 183.33333333 134.         105.66666667\n",
      "  71.33333333  97.33333333 175.33333333 196.66666667  70.\n",
      " 121.         138.66666667 401.33333333 141.33333333 178.33333333\n",
      "  68.33333333 113.33333333 136.         356.33333333 256.66666667\n",
      " 121.66666667 313.33333333 106.33333333 175.         123.\n",
      " 123.         160.         189.66666667 200.         132.66666667\n",
      " 116.         180.33333333 136.66666667 141.33333333 120.\n",
      " 209.66666667 188.33333333 150.         200.         209.66666667\n",
      "  62.         153.33333333  82.66666667  78.66666667 189.\n",
      " 134.33333333 107.33333333 114.33333333  62.         156.66666667\n",
      "  97.33333333 109.          55.         111.33333333 105.\n",
      " 149.66666667  88.66666667 135.33333333 189.66666667 140.\n",
      " 114.33333333 145.          76.66666667  93.33333333 126.66666667\n",
      " 123.66666667  93.33333333 426.66666667 130.         154.66666667\n",
      " 256.66666667 190.          92.66666667  87.33333333 121.66666667\n",
      " 204.66666667 128.33333333 138.33333333 126.66666667  53.\n",
      " 146.66666667 129.          98.         188.33333333 111.66666667\n",
      " 156.66666667 108.66666667 147.          66.66666667 153.33333333\n",
      " 199.33333333  93.33333333  93.33333333 148.33333333  55.\n",
      "  78.         119.66666667 112.66666667  54.33333333  71.66666667\n",
      " 189.66666667  56.33333333 150.         229.66666667 154.66666667\n",
      " 173.33333333 125.66666667 183.         171.33333333 115.\n",
      "  98.         177.66666667 140.33333333 100.         200.\n",
      " 136.66666667 346.33333333  93.33333333 109.         106.33333333\n",
      " 173.66666667 120.         146.66666667 181.33333333  56.33333333\n",
      " 127.33333333 107.33333333 127.33333333  84.66666667 143.\n",
      " 113.          56.66666667 131.66666667  88.33333333 212.33333333\n",
      " 145.66666667  68.33333333 136.66666667 108.33333333 313.33333333\n",
      "  71.         130.66666667 123.         222.66666667 129.\n",
      "  93.33333333  71.         145.66666667 213.33333333  96.33333333\n",
      " 172.          97.66666667  95.          88.66666667  40.\n",
      " 105.         110.66666667 143.33333333 129.33333333 166.33333333\n",
      " 162.66666667 120.         118.33333333 195.         129.\n",
      " 149.33333333 136.         134.         132.33333333 176.66666667\n",
      " 320.33333333 106.33333333 260.         121.66666667 190.\n",
      " 313.66666667 188.33333333 120.         100.          67.\n",
      "  61.66666667 140.         257.66666667 298.66666667  56.66666667\n",
      " 235.         115.         123.         213.33333333  99.\n",
      "  74.66666667  82.66666667 193.66666667 146.66666667 164.33333333\n",
      " 129.          53.         136.66666667 116.          90.\n",
      " 147.33333333 127.66666667  96.33333333 120.         172.\n",
      " 145.         527.33333333  87.66666667 101.33333333 190.\n",
      "  71.66666667  98.66666667 172.          73.         231.33333333\n",
      " 166.33333333 126.         191.66666667 118.33333333 143.33333333\n",
      " 210.         156.66666667  75.66666667 488.33333333 135.\n",
      " 136.66666667 171.33333333  56.66666667 123.33333333  64.\n",
      " 110.          96.         191.66666667  94.         115.\n",
      " 187.66666667 111.33333333 156.66666667  94.33333333  84.\n",
      " 134.66666667 178.66666667 106.33333333  98.         101.66666667\n",
      " 136.         104.         119.          91.66666667  86.33333333\n",
      "  57.         111.33333333  77.33333333 178.33333333 176.66666667\n",
      " 149.66666667  56.66666667 118.33333333 178.33333333 161.33333333\n",
      " 121.         132.33333333 115.         113.         122.33333333\n",
      "  93.33333333  90.          61.33333333  87.33333333 130.\n",
      " 199.33333333  93.          88.         195.         148.33333333\n",
      " 179.33333333  70.          71.66666667 190.          61.\n",
      " 378.33333333 238.33333333 200.         212.33333333 110.\n",
      " 238.          85.         136.66666667 105.          71.66666667\n",
      " 313.33333333  79.33333333 228.33333333 113.         140.\n",
      " 199.66666667  99.66666667 105.         134.66666667 189.66666667\n",
      " 128.33333333 108.33333333 200.         116.66666667 120.66666667\n",
      " 143.33333333 346.33333333 119.66666667 163.66666667 260.\n",
      " 194.66666667  61.         117.33333333  66.66666667  56.66666667\n",
      " 126.33333333 115.         130.         100.         179.66666667\n",
      " 113.         222.66666667 142.33333333 123.33333333 110.66666667\n",
      "  37.         132.66666667 118.33333333 346.33333333 114.33333333\n",
      " 129.33333333 248.33333333 107.         139.66666667  86.66666667\n",
      " 458.         148.33333333 183.33333333 120.66666667  95.\n",
      " 115.         179.66666667 116.66666667  46.33333333 126.66666667\n",
      " 608.         128.         139.          71.          45.66666667\n",
      " 119.66666667 106.66666667 135.33333333  48.33333333  64.\n",
      "  98.         149.33333333 121.33333333  54.33333333 105.\n",
      " 121.66666667 367.66666667  74.66666667 426.66666667  45.66666667\n",
      " 173.33333333 608.          75.         103.33333333 134.66666667\n",
      " 172.         130.         111.33333333 135.          49.66666667\n",
      " 105.         139.66666667 141.33333333 146.         126.\n",
      " 403.         105.         126.33333333 260.         108.33333333\n",
      " 108.33333333 187.         129.66666667  83.33333333 154.\n",
      "  85.         301.33333333 204.33333333 204.33333333  66.66666667\n",
      " 222.66666667  48.33333333 310.          89.66666667 112.66666667\n",
      "  90.33333333 183.         301.33333333 158.33333333 128.33333333\n",
      " 260.          98.33333333 164.33333333 135.66666667 114.33333333\n",
      " 163.33333333  84.         111.33333333  75.66666667 163.\n",
      "  81.          91.66666667 204.33333333  40.          85.33333333\n",
      " 150.33333333 149.66666667  98.66666667 112.66666667  52.\n",
      " 104.66666667 107.         319.66666667 125.         162.66666667\n",
      " 103.33333333 244.66666667 101.66666667  65.          40.\n",
      " 102.66666667 118.         118.66666667 313.66666667  98.66666667\n",
      " 149.33333333  96.         143.33333333  78.33333333 120.\n",
      " 156.66666667 115.33333333 102.66666667 202.66666667  69.33333333\n",
      "  46.33333333 193.66666667  84.          91.66666667 120.66666667\n",
      " 125.66666667  64.         393.          61.33333333 123.66666667\n",
      " 171.33333333 143.         181.33333333 260.          83.33333333\n",
      " 113.         106.33333333  63.         106.         310.\n",
      "  77.66666667 110.         207.66666667  93.33333333 212.33333333\n",
      " 241.66666667 119.66666667 133.33333333  87.33333333 100.66666667\n",
      " 207.66666667  97.         190.         218.33333333  50.\n",
      "  86.33333333 127.66666667 115.          97.         193.66666667\n",
      " 138.         136.         189.          85.33333333 116.66666667\n",
      " 100.66666667 367.66666667  99.          91.66666667 195.\n",
      " 179.66666667 180.66666667 248.33333333  55.          90.\n",
      " 208.33333333 149.33333333 112.33333333 196.33333333 135.33333333\n",
      " 179.66666667 367.66666667 238.33333333 116.         103.33333333\n",
      " 115.         126.33333333  71.33333333 140.         109.66666667\n",
      " 120.         195.         163.         200.         135.66666667\n",
      "  92.66666667 134.33333333  84.         148.         200.\n",
      " 150.         446.33333333 100.         135.66666667  77.66666667\n",
      " 476.66666667  54.         204.66666667 183.33333333 106.33333333\n",
      " 133.66666667 312.66666667 195.66666667 146.66666667  91.66666667\n",
      " 123.          93.33333333 179.66666667 173.33333333 121.\n",
      " 161.33333333  87.33333333 204.33333333 134.66666667 119.\n",
      "  72.33333333  77.33333333 291.66666667 199.33333333 123.33333333\n",
      " 268.          85.33333333  86.66666667 182.33333333  40.\n",
      "  97.         136.66666667 123.33333333 145.66666667 140.33333333\n",
      " 179.33333333 172.         118.33333333 135.66666667 183.33333333\n",
      " 105.66666667  77.66666667  70.         115.66666667 485.66666667\n",
      " 180.66666667 177.66666667  85.33333333 113.66666667 179.33333333\n",
      " 128.33333333 302.33333333 375.          97.         126.33333333\n",
      " 186.33333333 141.66666667 205.66666667 102.66666667  49.66666667\n",
      "  79.33333333 110.66666667  83.33333333 312.66666667 136.\n",
      "  87.66666667 153.         134.33333333  76.66666667 141.\n",
      "  77.          75.66666667  57.         115.         119.33333333\n",
      " 101.66666667 110.         166.66666667 156.66666667  71.66666667\n",
      "  68.33333333 193.33333333 201.33333333 113.         119.66666667\n",
      " 330.         157.33333333 120.         145.         115.\n",
      " 145.66666667 128.33333333 101.66666667 153.         148.33333333\n",
      " 139.         161.66666667  76.66666667  87.66666667 136.66666667\n",
      " 175.         151.33333333 253.         111.66666667 119.66666667\n",
      " 146.66666667 488.33333333 127.66666667 186.33333333 204.33333333\n",
      "  93.33333333  40.         127.33333333 145.          92.66666667\n",
      " 112.66666667  87.66666667 103.33333333 138.66666667 146.66666667\n",
      " 222.66666667 118.33333333 204.33333333  96.33333333 118.\n",
      " 110.66666667  99.66666667  99.33333333 298.33333333 161.66666667\n",
      " 115.         111.33333333  55.33333333  56.66666667 108.33333333\n",
      " 362.66666667  61.66666667  65.         104.66666667 171.33333333\n",
      " 142.66666667  53.         164.          92.          56.66666667\n",
      "  95.33333333 312.66666667 204.33333333 124.66666667  86.66666667\n",
      " 106.33333333 200.          96.33333333 149.66666667 101.66666667\n",
      "  86.66666667 298.66666667 202.66666667 129.         248.33333333\n",
      "  48.33333333 124.33333333 134.66666667 121.33333333 139.66666667\n",
      "  79.33333333 139.33333333 221.66666667 161.33333333 173.33333333\n",
      " 195.66666667 148.33333333 145.         123.33333333  85.\n",
      " 181.33333333 149.33333333 120.         186.33333333  54.\n",
      "  79.66666667  86.33333333 141.66666667 113.33333333 118.\n",
      " 106.33333333  76.66666667  64.         178.33333333 196.33333333\n",
      " 113.33333333 140.         229.66666667  99.66666667 375.\n",
      " 147.         195.         195.         123.         121.\n",
      " 313.33333333 149.66666667 256.66666667 204.33333333 212.33333333\n",
      " 125.          98.66666667 105.         145.         148.33333333\n",
      " 148.33333333 123.33333333 183.         186.33333333 210.\n",
      "  94.33333333 216.66666667 141.         112.66666667 150.\n",
      " 480.         106.         111.33333333  79.33333333  99.66666667\n",
      " 150.          68.33333333 139.33333333 194.66666667 181.33333333\n",
      " 159.66666667 117.66666667 118.66666667 187.66666667  96.33333333\n",
      " 121.         113.33333333 128.33333333 173.33333333 320.33333333\n",
      " 105.          76.66666667  94.         118.33333333  91.66666667\n",
      " 105.         488.33333333 238.33333333 175.33333333 166.33333333\n",
      " 161.66666667 608.         119.66666667 130.66666667 139.\n",
      "  83.33333333 118.66666667 110.66666667 121.66666667 135.\n",
      " 100.66666667 148.33333333  86.66666667 401.33333333 129.\n",
      " 112.66666667 121.33333333  79.33333333 200.         260.\n",
      " 150.         183.33333333 106.66666667  97.33333333  93.33333333\n",
      "  93.         446.33333333 127.33333333  63.          93.\n",
      " 106.66666667  76.66666667 136.66666667 146.66666667 121.66666667\n",
      " 105.         166.33333333 113.         149.33333333 488.33333333\n",
      "  96.33333333 123.         446.33333333 112.33333333  96.33333333\n",
      " 135.66666667 140.33333333 123.          66.         111.33333333\n",
      " 485.          93.33333333 185.66666667  87.33333333  95.\n",
      " 148.33333333  76.66666667 187.66666667 111.33333333 133.33333333\n",
      " 313.33333333  98.66666667 117.33333333 200.         116.\n",
      " 222.66666667  40.         145.         320.33333333  56.33333333\n",
      " 106.33333333 115.         310.          93.         218.66666667\n",
      " 182.33333333 100.          86.33333333 186.         118.33333333\n",
      " 485.          48.33333333  71.66666667  66.66666667 123.33333333\n",
      " 104.66666667 190.         135.66666667 218.33333333 139.66666667\n",
      "  99.         123.33333333  98.66666667 260.         193.33333333\n",
      "  99.33333333 113.         115.33333333 110.         164.33333333\n",
      " 188.33333333 121.          46.33333333 401.33333333 129.\n",
      "  55.          87.66666667 128.33333333 116.         189.66666667]\n"
     ]
    }
   ],
   "source": [
    "X_train1 = X_train[0:1000][numberCols].to_numpy()\n",
    "X_test1 = X_test[0:1000][numberCols].to_numpy()\n",
    "y_train1 = y_train[0:1000].to_numpy()[:,0]\n",
    "y_test1 = y_test[0:1000].to_numpy()[:,0]\n",
    "\n",
    "knnr = KNNRegressor(n_neighbors = 3, metric = 'euclidean', mode = 'uniform')\n",
    "knnr.fit(X_train1, y_train1)\n",
    "res = knnr.predict(X_test1)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2: Категориальные признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.1 (1 балл)</b>\n",
    "Реализуйте три функции расстояния на категориальных признаках, которые обсуждались на [третьем семинаре](https://github.com/mmp-mmro-team/mmp_mmro_fall_2019/blob/master/lecture-notes/Sem03_knn.pdf). Не забудьте, что KNNRegressor должен уметь работать с этими функциями расстояния. Как вариант, можно реализовать метрики как [user-defined distance](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train[0:1000][categoricalCols].to_numpy()\n",
    "X_test1 = X_test[0:1000][categoricalCols].to_numpy()\n",
    "y_train1 = y_train[0:1000].to_numpy()[:,0]\n",
    "y_test1 = y_test[0:1000].to_numpy()[:,0]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "X_train1[:, 0] = labelencoder.fit_transform(X_train1[:, 0])\n",
    "X_train1[:, 1] = labelencoder.fit_transform(X_train1[:, 1])\n",
    "X_train1[:, 2] = labelencoder.fit_transform(X_train1[:, 2])\n",
    "\n",
    "X_test1[:, 0] = labelencoder.fit_transform(X_test1[:, 0])\n",
    "X_test1[:, 1] = labelencoder.fit_transform(X_test1[:, 1])\n",
    "X_test1[:, 2] = labelencoder.fit_transform(X_test1[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.2 (1 балл)</b> Найдите все категориальные признаки в данных. Подсчитайте для каждой из метрик качество на тестовой выборке `X_test` при числе соседей $k = 10$. Качество измеряйте с помощью RMSE.\n",
    "\n",
    "Какая функция расстояния оказалась лучшей? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: overlap , k: 1\n",
      "Metric: overlap , k: 2\n",
      "Metric: overlap , k: 3\n",
      "Metric: overlap , k: 4\n",
      "Metric: overlap , k: 5\n",
      "Metric: overlap , k: 6\n",
      "Metric: overlap , k: 7\n",
      "Metric: overlap , k: 8\n",
      "Metric: overlap , k: 9\n",
      "Metric: overlap , k: 10\n",
      "Metric: overlap , k: 11\n",
      "Metric: overlap , k: 12\n",
      "Metric: overlap , k: 13\n",
      "Metric: overlap , k: 14\n",
      "Metric: overlap , k: 15\n",
      "Metric: overlap , k: 16\n",
      "Metric: overlap , k: 17\n",
      "Metric: overlap , k: 18\n",
      "Metric: overlap , k: 19\n",
      "Metric: overlap , k: 20\n",
      "Metric: overlap , k: 21\n",
      "Metric: overlap , k: 22\n",
      "Metric: overlap , k: 23\n",
      "Metric: overlap , k: 24\n",
      "Metric: overlap , k: 25\n",
      "Metric: overlap , k: 26\n",
      "Metric: overlap , k: 27\n",
      "Metric: overlap , k: 28\n",
      "Metric: overlap , k: 29\n",
      "Metric: overlap , k: 30\n",
      "Metric: overlap , k: 31\n",
      "Metric: overlap , k: 32\n",
      "Metric: overlap , k: 33\n",
      "Metric: overlap , k: 34\n",
      "Metric: overlap , k: 35\n",
      "Metric: flattened_overlap , k: 1\n",
      "Metric: flattened_overlap , k: 2\n",
      "Metric: flattened_overlap , k: 3\n",
      "Metric: flattened_overlap , k: 4\n",
      "Metric: flattened_overlap , k: 5\n",
      "Metric: flattened_overlap , k: 6\n",
      "Metric: flattened_overlap , k: 7\n",
      "Metric: flattened_overlap , k: 8\n",
      "Metric: flattened_overlap , k: 9\n",
      "Metric: flattened_overlap , k: 10\n",
      "Metric: flattened_overlap , k: 11\n",
      "Metric: flattened_overlap , k: 12\n",
      "Metric: flattened_overlap , k: 13\n",
      "Metric: flattened_overlap , k: 14\n",
      "Metric: flattened_overlap , k: 15\n",
      "Metric: flattened_overlap , k: 16\n",
      "Metric: flattened_overlap , k: 17\n",
      "Metric: flattened_overlap , k: 18\n",
      "Metric: flattened_overlap , k: 19\n",
      "Metric: flattened_overlap , k: 20\n",
      "Metric: flattened_overlap , k: 21\n",
      "Metric: flattened_overlap , k: 22\n",
      "Metric: flattened_overlap , k: 23\n",
      "Metric: flattened_overlap , k: 24\n",
      "Metric: flattened_overlap , k: 25\n",
      "Metric: flattened_overlap , k: 26\n",
      "Metric: flattened_overlap , k: 27\n",
      "Metric: flattened_overlap , k: 28\n",
      "Metric: flattened_overlap , k: 29\n",
      "Metric: flattened_overlap , k: 30\n",
      "Metric: flattened_overlap , k: 31\n",
      "Metric: flattened_overlap , k: 32\n",
      "Metric: flattened_overlap , k: 33\n",
      "Metric: flattened_overlap , k: 34\n",
      "Metric: flattened_overlap , k: 35\n",
      "Metric: log_overlap , k: 1\n",
      "Metric: log_overlap , k: 2\n",
      "Metric: log_overlap , k: 3\n",
      "Metric: log_overlap , k: 4\n",
      "Metric: log_overlap , k: 5\n",
      "Metric: log_overlap , k: 6\n",
      "Metric: log_overlap , k: 7\n",
      "Metric: log_overlap , k: 8\n",
      "Metric: log_overlap , k: 9\n",
      "Metric: log_overlap , k: 10\n",
      "Metric: log_overlap , k: 11\n",
      "Metric: log_overlap , k: 12\n",
      "Metric: log_overlap , k: 13\n",
      "Metric: log_overlap , k: 14\n",
      "Metric: log_overlap , k: 15\n",
      "Metric: log_overlap , k: 16\n",
      "Metric: log_overlap , k: 17\n",
      "Metric: log_overlap , k: 18\n",
      "Metric: log_overlap , k: 19\n",
      "Metric: log_overlap , k: 20\n",
      "Metric: log_overlap , k: 21\n",
      "Metric: log_overlap , k: 22\n",
      "Metric: log_overlap , k: 23\n",
      "Metric: log_overlap , k: 24\n",
      "Metric: log_overlap , k: 25\n",
      "Metric: log_overlap , k: 26\n",
      "Metric: log_overlap , k: 27\n",
      "Metric: log_overlap , k: 28\n",
      "Metric: log_overlap , k: 29\n",
      "Metric: log_overlap , k: 30\n",
      "Metric: log_overlap , k: 31\n",
      "Metric: log_overlap , k: 32\n",
      "Metric: log_overlap , k: 33\n",
      "Metric: log_overlap , k: 34\n",
      "Metric: log_overlap , k: 35\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Ваш код здесь\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k_max = 35\n",
    "metrics_array = ['overlap', 'flattened_overlap', 'log_overlap']\n",
    "res = np.zeros(k_max * len(metrics_array)).reshape(k_max, len(metrics_array))\n",
    "for m in range(len(metrics_array)):\n",
    "    for k in range(0, k_max):\n",
    "        print(\"Metric: %s , k: %d\" % (metrics_array[m], k + 1))\n",
    "        knnr = KNNRegressor(n_neighbors = k + 1, metric = metrics_array[m], mode = 'distance')\n",
    "        knnr.fit(X_train1, y_train1)\n",
    "        pred = knnr.predict(X_test1)\n",
    "        res[k][m] = np.sqrt(mean_squared_error(y_test1, pred))\n",
    "\n",
    "plt.plot(res)\n",
    "plt.legend(metrics_array, loc='upper right')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.3 (1 балл) бонус</b> Подберите лучшее (на тестовой выборке) число соседей $k$ для каждой из функций расстояния. Какого удалось достичь уровня качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для overlap оптимальное значение k = 20 (результат 293.4341)\n",
      "Для flattened_overlap оптимальное значение k = 20 (результат 293.4594)\n",
      "Для log_overlap оптимальное значение k = 20 (результат 293.4594)\n"
     ]
    }
   ],
   "source": [
    "# Ваш код здесь\n",
    "metrics_array = ['overlap', 'flattened_overlap', 'log_overlap']\n",
    "for m in range(len(metrics_array)):\n",
    "    k = np.argmin(res[:,m])\n",
    "    print(\"Для %s оптимальное значение k = %d (результат %.4f)\" % (metrics_array[m], k, res[k,m]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.4 (2.5 балла)</b> Отойдем ненадолго от задачи регрессии и перейдём к задаче классификации: будем определять, являеться ли квартира дорогой $(target = 1)$ или дешевой $(target = 0)$. Будем считать дорогими квариры, цена которых выше среднего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = (data.price > data.price.mean()).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте счетчики, которые заменят категориальные признаки на вещественные.\n",
    "\n",
    "А именно, для каждого категориального признака $f_j(x)$ необходимо сделать следующее:\n",
    "1. Число `counts` объектов в обучающей выборке с таким же значением признака.\n",
    "\\begin{align}\n",
    "counts_j(c) = \\sum_{i=1}^l [f_j(x_i) = c]\n",
    "\\end{align}\n",
    "2. Число `successes` объектов первого класса ($y = 1$) в обучающей выборке с таким же значением признака.\n",
    "\\begin{align}\n",
    "successes_j(c) = \\sum_{i=1}^l[f_j(x_i) = c][y_i = +1].\n",
    "\\end{align}\n",
    "3. Сглаженное отношение двух предыдущих величин:\n",
    "\\begin{align}\n",
    "p_j(c) = \\frac{successes_j(c) + a}{counts_j(c) + b},\n",
    "\\end{align}\n",
    "\n",
    "где $a$ и $b$ - априорные счетчики (например, a = 1, b = 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 3, 'b': 1, 'c': 1, 'd': 2, 'e': 1, 'f': 1}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def counters(x):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        x: value on categorical feature for N objects\n",
    "    returns: vector of length N\n",
    "    \"\"\"\n",
    "    # Ваш код здесь\n",
    "    pass\n",
    "\n",
    "def counts(X: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    params:\n",
    "        x: value on categorical feature for N objects\n",
    "    returns: vector of length N\n",
    "    \"\"\"\n",
    "    # Ваш код здесь\n",
    "    res = {}\n",
    "    unique, counts = np.unique(X, return_counts = True)\n",
    "    return dict(np.asarray((unique, counts)).T)\n",
    "\n",
    "def successes(X: np.array, Y: np.array) -> np.array:\n",
    "    res = {}\n",
    "    \n",
    "    unique = np.unique(X, return_counts = False)\n",
    "    for val in unique:\n",
    "        count = 0\n",
    "        for row in range(len(X)):\n",
    "            if ((X[row] == val) and X[row]):\n",
    "                count = count + 1\n",
    "        res[val] = count\n",
    "    \n",
    "    return res\n",
    "\n",
    "# counts(['a','b','c','a','e','d','d','f','a'])\n",
    "successes(['a','b','c','a','e','d','d','f','a'], [1,0,1,1,1,0,0,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку признаки, содержащие информацию о целевой переменной, могут привести к переобучению, может оказаться полезным сделать *фолдинг*: разбить обучающую выборку на $n$ частей, и для $i$-й части считать `counts` и `successes` по всем остальным частям. Для тестовой выборки используются счетчики, посчитанный по всей обучающей выборке. Реализуйте и такой вариант. Достаточно взять $n = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fold_counters(x):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        x: value on categorical feature for N objects\n",
    "    returns: vector of length N\n",
    "    \"\"\"\n",
    "    # Ваш код здесь\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитайте на тесте AUC-ROC метода $k$ ближайших соседей с евклидовой метрикой для выборки, где категориальные признаки заменены на счетчики. Сравните по AUC-ROC два варианта формирования выборки — с фолдингом и без. Не забудьте подобрать наилучшее число соседей $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.5 (1 балл)</b> Вернемся к задаче регрессии. Утверждается, что для задачи регрессии можно также сделать преобразование категориальных признаков в действительные числа. Для этого достаточно для каждого значения признака $f_j$ вычислить:\n",
    "\\begin{align}\n",
    "p_j(c) = g(T_i | f_j(x_i) = c),\n",
    "\\end{align}\n",
    "\n",
    "где $T_i$ - значения целевой переменной объекта $x_i$. Функция $g$ - среднее (mean) или среднеквадратичное отклонение (std).\n",
    "\n",
    "Закодируйте категориальные признаки обоими способами и найдите значение RMSE. Используйте евклидову метрику для поиска ближайших соседей. Для какой функции $g$ значение RMSE лучше? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3: Текстовые признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3.1 (2 балла)</b> Перейдем от категориальным признаков к текстовым. Рассмотрим 2 способа преобразования текста в действительные числа:\n",
    "- Мешок слов (Bag of Words)\n",
    "- TF-IDF\n",
    "\n",
    "[Здесь](https://scikit-learn.org/stable/modules/feature_extraction.html) вы можете прочитать про их применение в Питоне.\n",
    "\n",
    "Сравните оба способа на задаче регресси. Какую лучше метрику использовать: евклидову или косинусную меру? Постройте графики зависимости качества решения задачи от способа преобразования, метрики и количества соседей. Мера качества - RMSE.\n",
    "\n",
    "Объясните полученные результаты.\n",
    "\n",
    "Перед преобразованием не забудьте уменьшить размер словаря. Например, это можно сделать за счет приведения всех слов к одному регистру и удаления [стопслов](https://en.wikipedia.org/wiki/Stop_words) (артиклей, предлогов, союзов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3.2 (1 балл)</b> Используя все доступные признаки, решите задачу регрессии. Для категориальных и текстовых признаков выберите лучшие преобразования. Повлияло ли добавление количественного признака на метрику качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 4: Выводы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваши выводы здесь (ノ°∀°)ノ⌒･*:.｡. .｡.:*･゜ﾟ･*☆"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
