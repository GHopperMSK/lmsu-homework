{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Машинное обучение, ВМК МГУ\n",
    "\n",
    "## Практическое задание 2\n",
    "\n",
    "### Общая информация\n",
    "Дата выдачи: 9 октября 2019\n",
    "\n",
    "Максимальная оценка: 10 баллов + 1 бонусный балл\n",
    "\n",
    "Мягкий дедлайн: 23:59MSK 23 октября (за каждый день просрочки снимается 1 балл)\n",
    "\n",
    "Жесткий дедлайн: 23:59MSK 30 октября."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "В этом задании вы:\n",
    "- Познакомитесь с методом решения задачи регрессии на основе метода ближайших соседей.\n",
    "- Реализуете алгоритм kNN для задачи регрессии.\n",
    "- Изучите методы работы с категориальными и текстовыми переменными.\n",
    "\n",
    "\n",
    "### Оценивание и штрафы\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов.\n",
    "\n",
    "Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "\n",
    "\n",
    "### Формат сдачи\n",
    "Для сдачи задания переименуйте получившийся файл *.ipynb в соответствии со следующим форматом: homework-practice-02-Username.ipynb, где Username — ваша фамилия и имя на латинице именно в таком порядке (например, homework-practice-02-ivanov.ipynb).\n",
    "\n",
    "Далее отправьте этот файл на anytask в соответсвующий раздел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все эксперименты в этой лабораторной работе предлагается проводить на данных соревнования New York City Airbnb Open Data: https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data#AB_NYC_2019.csv\n",
    "\n",
    "В данной задаче предлагается предсказать цену на съем квартиры в зависимости от её параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_name</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>room_type</th>\n",
       "      <th>price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>last_review</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2539</td>\n",
       "      <td>Clean &amp; quiet apt home by the park</td>\n",
       "      <td>2787</td>\n",
       "      <td>John</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Kensington</td>\n",
       "      <td>40.64749</td>\n",
       "      <td>-73.97237</td>\n",
       "      <td>Private room</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2018-10-19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>6</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2595</td>\n",
       "      <td>Skylit Midtown Castle</td>\n",
       "      <td>2845</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Midtown</td>\n",
       "      <td>40.75362</td>\n",
       "      <td>-73.98377</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2</td>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3647</td>\n",
       "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
       "      <td>4632</td>\n",
       "      <td>Elisabeth</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Harlem</td>\n",
       "      <td>40.80902</td>\n",
       "      <td>-73.94190</td>\n",
       "      <td>Private room</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3831</td>\n",
       "      <td>Cozy Entire Floor of Brownstone</td>\n",
       "      <td>4869</td>\n",
       "      <td>LisaRoxanne</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Clinton Hill</td>\n",
       "      <td>40.68514</td>\n",
       "      <td>-73.95976</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>270</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>4.64</td>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5022</td>\n",
       "      <td>Entire Apt: Spacious Studio/Loft by central park</td>\n",
       "      <td>7192</td>\n",
       "      <td>Laura</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>East Harlem</td>\n",
       "      <td>40.79851</td>\n",
       "      <td>-73.94399</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              name  host_id  \\\n",
       "0  2539                Clean & quiet apt home by the park     2787   \n",
       "1  2595                             Skylit Midtown Castle     2845   \n",
       "2  3647               THE VILLAGE OF HARLEM....NEW YORK !     4632   \n",
       "3  3831                   Cozy Entire Floor of Brownstone     4869   \n",
       "4  5022  Entire Apt: Spacious Studio/Loft by central park     7192   \n",
       "\n",
       "     host_name neighbourhood_group neighbourhood  latitude  longitude  \\\n",
       "0         John            Brooklyn    Kensington  40.64749  -73.97237   \n",
       "1     Jennifer           Manhattan       Midtown  40.75362  -73.98377   \n",
       "2    Elisabeth           Manhattan        Harlem  40.80902  -73.94190   \n",
       "3  LisaRoxanne            Brooklyn  Clinton Hill  40.68514  -73.95976   \n",
       "4        Laura           Manhattan   East Harlem  40.79851  -73.94399   \n",
       "\n",
       "         room_type  price  minimum_nights  number_of_reviews last_review  \\\n",
       "0     Private room    149               1                  9  2018-10-19   \n",
       "1  Entire home/apt    225               1                 45  2019-05-21   \n",
       "2     Private room    150               3                  0         NaN   \n",
       "3  Entire home/apt     89               1                270  2019-07-05   \n",
       "4  Entire home/apt     80              10                  9  2018-11-19   \n",
       "\n",
       "   reviews_per_month  calculated_host_listings_count  availability_365  \n",
       "0               0.21                               6               365  \n",
       "1               0.38                               2               355  \n",
       "2                NaN                               1               365  \n",
       "3               4.64                               1               194  \n",
       "4               0.10                               1                 0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('AB_NYC_2019.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48895, 16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 48895\n",
      "name 47906\n",
      "host_id 37457\n",
      "host_name 11453\n",
      "neighbourhood_group 5\n",
      "neighbourhood 221\n",
      "latitude 19048\n",
      "longitude 14718\n",
      "room_type 3\n",
      "price 674\n",
      "minimum_nights 109\n",
      "number_of_reviews 394\n",
      "last_review 1765\n",
      "reviews_per_month 938\n",
      "calculated_host_listings_count 47\n",
      "availability_365 366\n"
     ]
    }
   ],
   "source": [
    "# число значений у признаков\n",
    "for col_name in data.columns:\n",
    "    print(col_name, len(data[col_name].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                    0\n",
       "name                                 16\n",
       "host_id                               0\n",
       "host_name                            21\n",
       "neighbourhood_group                   0\n",
       "neighbourhood                         0\n",
       "latitude                              0\n",
       "longitude                             0\n",
       "room_type                             0\n",
       "price                                 0\n",
       "minimum_nights                        0\n",
       "number_of_reviews                     0\n",
       "last_review                       10052\n",
       "reviews_per_month                 10052\n",
       "calculated_host_listings_count        0\n",
       "availability_365                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видите, в данных есть пропуски. Не забудьте обработать их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь (ノ°∀°)ノ⌒･*:.｡. .｡.:*･゜ﾟ･*☆\n",
    "numberCols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numberCols.remove('id')\n",
    "numberCols.remove('price')\n",
    "\n",
    "categoricalCols = ['neighbourhood_group', 'neighbourhood', 'room_type']\n",
    "\n",
    "data[numberCols] = data[numberCols].fillna(0)\n",
    "data[categoricalCols] = data[categoricalCols].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобъем данные на обучение и контроль."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['price']), data[['price']],\n",
    "                                                    test_size=0.3, random_state=241)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1: Алгоритм kNN в задаче регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1.1 (1.5 балла) </b>\n",
    "Реализуйте класс `KNNRegressor`, который используя метод k ближайших соседей решает задачу регрессии. Для решение данной задачи, необходимо найти $N_k$ - k соседей, и после использовать значения их целевых переменных для предсказания:\n",
    "\\begin{align}\n",
    "y = \\frac{1}{k}\\sum_{n \\in N_k}w_n y_n,\n",
    "\\end{align}\n",
    "\n",
    "где $w_n$ - вес каждого соседа. \n",
    "\n",
    "При этом `KNNRegressor` может работать в 2 режимах:\n",
    " - $uniform$ - ближайшие соседи учитываются с одинаковыми весами.\n",
    " - $distance$ - вес ближайших соседей зависит от расстояния\n",
    " \n",
    "Сигнатуру методов при желании можно менять."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Callable, Iterable, Optional\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "class KNNRegressor:\n",
    "    def __init__(self, n_neighbors: int, metric: Union[str, Callable], mode: str = 'uniform'):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            n_neighbors: number of neighbors\n",
    "            metric: metric to use for distance computation\n",
    "            mode: 'uniform' or 'distance'\n",
    "            'uniform' - all points in each neighborhood are weighted equally\n",
    "            'distance' - weight points by the inverse of their distance\n",
    "        \"\"\"\n",
    "#         self.__nn = NearestNeighbors(n_neighbors = n_neighbors, metric = metric)\n",
    "#         self.__nnTest = KNeighborsRegressor(n_neighbors = n_neighbors, metric = metric, weights = mode)\n",
    "        self.__metric = metric\n",
    "        self.__mode = mode\n",
    "        self.__n_neighbors = n_neighbors\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array) -> None:\n",
    "        \"\"\"\n",
    "            X: data\n",
    "            y: labels\n",
    "        \"\"\"\n",
    "        # Ваш код здесь\n",
    "        self.__X = X\n",
    "        self.__y = y\n",
    "#         self.__nn.fit(X, y)\n",
    "#         self.__nnTest.fit(X, y)\n",
    "        \n",
    "    def euclidean_distance(self, X, Y):\n",
    "        d = len(X[0])\n",
    "        res = np.zeros(len(X)*len(Y)).reshape(len(X), len(Y))\n",
    "        for x in range(len(X)):\n",
    "            for y in range(len(Y)):\n",
    "                sum = 0\n",
    "                for f in range(len(Y[y])):\n",
    "                    sum += (X[x][f] - Y[y][f])**2\n",
    "                res[x][y] = np.sqrt(sum)\n",
    "        return res;\n",
    "\n",
    "    def cosine_distance(self, X, Y):\n",
    "        sumyy = (Y**2).sum(1)\n",
    "        sumxx = (X**2).sum(1, keepdims=1)\n",
    "        sumxy = X.dot(Y.T)\n",
    "        return 1 - (sumxy / np.sqrt(sumxx)) / np.sqrt(sumyy)\n",
    "    \n",
    "    def overlap(self, X, Y):\n",
    "        res = []\n",
    "        res = np.zeros(len(X)*len(Y)).reshape(len(X), len(Y))\n",
    "        for x in range(len(X)):\n",
    "            for y in range(len(Y)):\n",
    "                dist = 0\n",
    "                for f in range(len(Y[y])):\n",
    "                    if (X[x][f] != Y[y][f]):\n",
    "                        dist = 1\n",
    "                        break\n",
    "                res[x][y] = dist\n",
    "        return res;\n",
    "\n",
    "    def flattened_overlap(self, X, Y):\n",
    "        # находим частоту признаков\n",
    "        freqs = {}\n",
    "        for col in range(X.shape[1]):\n",
    "            values, freq = np.unique(X[:,col], return_counts=True)\n",
    "            freqs[col] = {values[q]: freq[q] for q in range(len(values))} \n",
    "        \n",
    "        res = np.zeros(len(X)*len(Y)).reshape(len(X), len(Y))\n",
    "        for x in range(len(X)):\n",
    "            for y in range(len(Y)):\n",
    "                dist = 0\n",
    "                for с in range(len(Y[y])):\n",
    "                    if (X[x][с] != Y[y][с]):\n",
    "                        dist = dist + freqs[с][X[x][с]]\n",
    "                \n",
    "                res[x][y] = dist\n",
    "        return res;\n",
    "    \n",
    "    def log_overlap(self, X, Y):\n",
    "        \n",
    "        # находим частоту признаков\n",
    "        freqsX = {}\n",
    "        for col in range(X.shape[1]):\n",
    "            values, freq = np.unique(X[:,col], return_counts=True)\n",
    "            freqsX[col] = {values[q]: freq[q] for q in range(len(values))} \n",
    "\n",
    "        freqsY = {}\n",
    "        for col in range(Y.shape[1]):\n",
    "            values, freq = np.unique(Y[:,col], return_counts=True)\n",
    "            freqsY[col] = {values[q]: freq[q] for q in range(len(values))} \n",
    "        \n",
    "        res = np.zeros(len(X)*len(Y)).reshape(len(X), len(Y))\n",
    "        for x in range(len(X)):\n",
    "            for y in range(len(Y)):\n",
    "                dist = 0\n",
    "                for f in range(len(Y[y])):\n",
    "                    if (X[x][f] != Y[y][f]):\n",
    "                        dist = dist + np.log(1 + freqsX[col][X[x][f]]) * np.log(1 + freqsY[col][Y[x][f]])\n",
    "                \n",
    "                res[x][y] = dist\n",
    "        return res;\n",
    "\n",
    "    def find_kneighbors(self, X, return_distance = True):\n",
    "        \n",
    "        if (self.__metric == 'euclidean'):\n",
    "            dsModRes = self.euclidean_distance(X, self.__X)\n",
    "        elif (self.__metric == 'cosine'):\n",
    "            dsModRes = self.cosine_distance(X, self.__X)\n",
    "        elif (self.__metric == 'overlap'):\n",
    "            dsModRes = self.overlap(X, self.__X)\n",
    "        elif (self.__metric == 'flattened_overlap'):\n",
    "            dsModRes = self.flattened_overlap(X, self.__X)\n",
    "        elif (self.__metric == 'log_overlap'):\n",
    "            dsModRes = self.flattened_overlap(X, self.__X)\n",
    "        else:\n",
    "            raise Exception(\"Unknown 'metric' param value\")\n",
    "\n",
    "        res = []\n",
    "        dists = []\n",
    "        for i in range(len(dsModRes)):\n",
    "            tmpDists = np.argsort(dsModRes[i])[:self.__n_neighbors]\n",
    "            res.append(dsModRes[i][tmpDists])\n",
    "            if (return_distance):\n",
    "                dists.append(tmpDists)\n",
    "\n",
    "        if (return_distance):\n",
    "            return (res, dists)\n",
    "        else:\n",
    "            return (res)\n",
    "        \n",
    "    def predict(self, X: np.array, n_neighbors: Optional[int] = None) -> np.array:\n",
    "        \"\"\"\n",
    "            X: data\n",
    "            n_neighbors: number of neighbors\n",
    "        \"\"\"\n",
    "        # Ваш код здесь\n",
    "#         distances, indices = self.__nn.kneighbors(X)\n",
    "#         distances, indices = self.__nnTest.kneighbors(X)\n",
    "        distances, indices = self.find_kneighbors(X)\n",
    "    \n",
    "        if (self.__mode == 'uniform'):\n",
    "            res = self.__y[indices].mean(axis=1)\n",
    "        else: \n",
    "            res = []\n",
    "            for row in range(len(indices)):\n",
    "                predictedVal = 0\n",
    "                weightsSum = 0\n",
    "                for col in range(len(indices[row])):\n",
    "                    w = 1 / (1 + distances[row][col])\n",
    "                    weightsSum = weightsSum + w\n",
    "                    predictedVal = predictedVal + w * self.__y[indices[row][col]]\n",
    "                res.append(predictedVal / weightsSum)\n",
    "            res = np.array(res)\n",
    "#         return self.__nnTest.predict(X)\n",
    "        return res\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 86.66666667 333.33333333 260.         154.66666667 125.33333333\n",
      "  70.         199.66666667 181.33333333 333.33333333  75.66666667\n",
      " 241.66666667 140.          86.66666667  87.66666667 218.66666667\n",
      "  88.66666667  90.33333333 130.         186.66666667 113.\n",
      " 187.66666667 186.         195.          90.          86.66666667\n",
      " 119.66666667  79.33333333  63.         130.         120.\n",
      " 193.33333333 115.66666667  63.         111.         114.33333333\n",
      " 183.33333333  56.33333333 129.33333333 183.33333333 212.33333333\n",
      " 130.          68.33333333 176.66666667 319.66666667 104.66666667\n",
      " 164.33333333 143.33333333  65.66666667 111.66666667  77.66666667\n",
      " 270.         146.66666667 101.66666667 134.66666667 139.33333333\n",
      " 129.          61.33333333  99.33333333 149.66666667  62.\n",
      "  71.66666667  74.66666667  68.33333333  98.66666667  57.33333333\n",
      " 113.33333333  85.66666667  98.         114.33333333 240.\n",
      " 178.33333333  84.66666667 105.66666667  88.33333333 187.66666667\n",
      " 110.66666667 111.33333333 175.          63.          71.\n",
      " 123.33333333  99.         116.66666667 135.66666667 175.33333333\n",
      "  68.33333333  99.33333333  63.         112.66666667  99.33333333\n",
      "  99.33333333 123.         194.66666667 123.          93.33333333\n",
      " 108.33333333 148.33333333 113.33333333 270.         126.\n",
      " 134.66666667  84.          91.66666667 228.         133.33333333\n",
      " 151.33333333  91.66666667 183.33333333 134.         105.66666667\n",
      "  71.33333333  97.33333333 175.33333333 196.66666667  70.\n",
      " 121.         138.66666667 401.33333333 141.33333333 178.33333333\n",
      "  68.33333333 113.33333333 136.         356.33333333 256.66666667\n",
      " 121.66666667 313.33333333 106.33333333 175.         123.\n",
      " 123.         160.         189.66666667 200.         132.66666667\n",
      " 116.         180.33333333 136.66666667 141.33333333 120.\n",
      " 209.66666667 188.33333333 150.         200.         209.66666667\n",
      "  62.         153.33333333  82.66666667  78.66666667 189.\n",
      " 134.33333333 107.33333333 114.33333333  62.         156.66666667\n",
      "  97.33333333 109.          55.         111.33333333 105.\n",
      " 149.66666667  88.66666667 135.33333333 189.66666667 140.\n",
      " 114.33333333 145.          76.66666667  93.33333333 126.66666667\n",
      " 123.66666667  93.33333333 426.66666667 130.         154.66666667\n",
      " 256.66666667 190.          92.66666667  87.33333333 121.66666667\n",
      " 204.66666667 128.33333333 138.33333333 126.66666667  53.\n",
      " 146.66666667 129.          98.         188.33333333 111.66666667\n",
      " 156.66666667 108.66666667 147.          66.66666667 153.33333333\n",
      " 199.33333333  93.33333333  93.33333333 148.33333333  55.\n",
      "  78.         119.66666667 112.66666667  54.33333333  71.66666667\n",
      " 189.66666667  56.33333333 150.         229.66666667 154.66666667\n",
      " 173.33333333 125.66666667 183.         171.33333333 115.\n",
      "  98.         177.66666667 140.33333333 100.         200.\n",
      " 136.66666667 346.33333333  93.33333333 109.         106.33333333\n",
      " 173.66666667 120.         146.66666667 181.33333333  56.33333333\n",
      " 127.33333333 107.33333333 127.33333333  84.66666667 143.\n",
      " 113.          56.66666667 131.66666667  88.33333333 212.33333333\n",
      " 145.66666667  68.33333333 136.66666667 108.33333333 313.33333333\n",
      "  71.         130.66666667 123.         222.66666667 129.\n",
      "  93.33333333  71.         145.66666667 213.33333333  96.33333333\n",
      " 172.          97.66666667  95.          88.66666667  40.\n",
      " 105.         110.66666667 143.33333333 129.33333333 166.33333333\n",
      " 162.66666667 120.         118.33333333 195.         129.\n",
      " 149.33333333 136.         134.         132.33333333 176.66666667\n",
      " 320.33333333 106.33333333 260.         121.66666667 190.\n",
      " 313.66666667 188.33333333 120.         100.          67.\n",
      "  61.66666667 140.         257.66666667 298.66666667  56.66666667\n",
      " 235.         115.         123.         213.33333333  99.\n",
      "  74.66666667  82.66666667 193.66666667 146.66666667 164.33333333\n",
      " 129.          53.         136.66666667 116.          90.\n",
      " 147.33333333 127.66666667  96.33333333 120.         172.\n",
      " 145.         527.33333333  87.66666667 101.33333333 190.\n",
      "  71.66666667  98.66666667 172.          73.         231.33333333\n",
      " 166.33333333 126.         191.66666667 118.33333333 143.33333333\n",
      " 210.         156.66666667  75.66666667 488.33333333 135.\n",
      " 136.66666667 171.33333333  56.66666667 123.33333333  64.\n",
      " 110.          96.         191.66666667  94.         115.\n",
      " 187.66666667 111.33333333 156.66666667  94.33333333  84.\n",
      " 134.66666667 178.66666667 106.33333333  98.         101.66666667\n",
      " 136.         104.         119.          91.66666667  86.33333333\n",
      "  57.         111.33333333  77.33333333 178.33333333 176.66666667\n",
      " 149.66666667  56.66666667 118.33333333 178.33333333 161.33333333\n",
      " 121.         132.33333333 115.         113.         122.33333333\n",
      "  93.33333333  90.          61.33333333  87.33333333 130.\n",
      " 199.33333333  93.          88.         195.         148.33333333\n",
      " 179.33333333  70.          71.66666667 190.          61.\n",
      " 378.33333333 238.33333333 200.         212.33333333 110.\n",
      " 238.          85.         136.66666667 105.          71.66666667\n",
      " 313.33333333  79.33333333 228.33333333 113.         140.\n",
      " 199.66666667  99.66666667 105.         134.66666667 189.66666667\n",
      " 128.33333333 108.33333333 200.         116.66666667 120.66666667\n",
      " 143.33333333 346.33333333 119.66666667 163.66666667 260.\n",
      " 194.66666667  61.         117.33333333  66.66666667  56.66666667\n",
      " 126.33333333 115.         130.         100.         179.66666667\n",
      " 113.         222.66666667 142.33333333 123.33333333 110.66666667\n",
      "  37.         132.66666667 118.33333333 346.33333333 114.33333333\n",
      " 129.33333333 248.33333333 107.         139.66666667  86.66666667\n",
      " 458.         148.33333333 183.33333333 120.66666667  95.\n",
      " 115.         179.66666667 116.66666667  46.33333333 126.66666667\n",
      " 608.         128.         139.          71.          45.66666667\n",
      " 119.66666667 106.66666667 135.33333333  48.33333333  64.\n",
      "  98.         149.33333333 121.33333333  54.33333333 105.\n",
      " 121.66666667 367.66666667  74.66666667 426.66666667  45.66666667\n",
      " 173.33333333 608.          75.         103.33333333 134.66666667\n",
      " 172.         130.         111.33333333 135.          49.66666667\n",
      " 105.         139.66666667 141.33333333 146.         126.\n",
      " 403.         105.         126.33333333 260.         108.33333333\n",
      " 108.33333333 187.         129.66666667  83.33333333 154.\n",
      "  85.         301.33333333 204.33333333 204.33333333  66.66666667\n",
      " 222.66666667  48.33333333 310.          89.66666667 112.66666667\n",
      "  90.33333333 183.         301.33333333 158.33333333 128.33333333\n",
      " 260.          98.33333333 164.33333333 135.66666667 114.33333333\n",
      " 163.33333333  84.         111.33333333  75.66666667 163.\n",
      "  81.          91.66666667 204.33333333  40.          85.33333333\n",
      " 150.33333333 149.66666667  98.66666667 112.66666667  52.\n",
      " 104.66666667 107.         319.66666667 125.         162.66666667\n",
      " 103.33333333 244.66666667 101.66666667  65.          40.\n",
      " 102.66666667 118.         118.66666667 313.66666667  98.66666667\n",
      " 149.33333333  96.         143.33333333  78.33333333 120.\n",
      " 156.66666667 115.33333333 102.66666667 202.66666667  69.33333333\n",
      "  46.33333333 193.66666667  84.          91.66666667 120.66666667\n",
      " 125.66666667  64.         393.          61.33333333 123.66666667\n",
      " 171.33333333 143.         181.33333333 260.          83.33333333\n",
      " 113.         106.33333333  63.         106.         310.\n",
      "  77.66666667 110.         207.66666667  93.33333333 212.33333333\n",
      " 241.66666667 119.66666667 133.33333333  87.33333333 100.66666667\n",
      " 207.66666667  97.         190.         218.33333333  50.\n",
      "  86.33333333 127.66666667 115.          97.         193.66666667\n",
      " 138.         136.         189.          85.33333333 116.66666667\n",
      " 100.66666667 367.66666667  99.          91.66666667 195.\n",
      " 179.66666667 180.66666667 248.33333333  55.          90.\n",
      " 208.33333333 149.33333333 112.33333333 196.33333333 135.33333333\n",
      " 179.66666667 367.66666667 238.33333333 116.         103.33333333\n",
      " 115.         126.33333333  71.33333333 140.         109.66666667\n",
      " 120.         195.         163.         200.         135.66666667\n",
      "  92.66666667 134.33333333  84.         148.         200.\n",
      " 150.         446.33333333 100.         135.66666667  77.66666667\n",
      " 476.66666667  54.         204.66666667 183.33333333 106.33333333\n",
      " 133.66666667 312.66666667 195.66666667 146.66666667  91.66666667\n",
      " 123.          93.33333333 179.66666667 173.33333333 121.\n",
      " 161.33333333  87.33333333 204.33333333 134.66666667 119.\n",
      "  72.33333333  77.33333333 291.66666667 199.33333333 123.33333333\n",
      " 268.          85.33333333  86.66666667 182.33333333  40.\n",
      "  97.         136.66666667 123.33333333 145.66666667 140.33333333\n",
      " 179.33333333 172.         118.33333333 135.66666667 183.33333333\n",
      " 105.66666667  77.66666667  70.         115.66666667 485.66666667\n",
      " 180.66666667 177.66666667  85.33333333 113.66666667 179.33333333\n",
      " 128.33333333 302.33333333 375.          97.         126.33333333\n",
      " 186.33333333 141.66666667 205.66666667 102.66666667  49.66666667\n",
      "  79.33333333 110.66666667  83.33333333 312.66666667 136.\n",
      "  87.66666667 153.         134.33333333  76.66666667 141.\n",
      "  77.          75.66666667  57.         115.         119.33333333\n",
      " 101.66666667 110.         166.66666667 156.66666667  71.66666667\n",
      "  68.33333333 193.33333333 201.33333333 113.         119.66666667\n",
      " 330.         157.33333333 120.         145.         115.\n",
      " 145.66666667 128.33333333 101.66666667 153.         148.33333333\n",
      " 139.         161.66666667  76.66666667  87.66666667 136.66666667\n",
      " 175.         151.33333333 253.         111.66666667 119.66666667\n",
      " 146.66666667 488.33333333 127.66666667 186.33333333 204.33333333\n",
      "  93.33333333  40.         127.33333333 145.          92.66666667\n",
      " 112.66666667  87.66666667 103.33333333 138.66666667 146.66666667\n",
      " 222.66666667 118.33333333 204.33333333  96.33333333 118.\n",
      " 110.66666667  99.66666667  99.33333333 298.33333333 161.66666667\n",
      " 115.         111.33333333  55.33333333  56.66666667 108.33333333\n",
      " 362.66666667  61.66666667  65.         104.66666667 171.33333333\n",
      " 142.66666667  53.         164.          92.          56.66666667\n",
      "  95.33333333 312.66666667 204.33333333 124.66666667  86.66666667\n",
      " 106.33333333 200.          96.33333333 149.66666667 101.66666667\n",
      "  86.66666667 298.66666667 202.66666667 129.         248.33333333\n",
      "  48.33333333 124.33333333 134.66666667 121.33333333 139.66666667\n",
      "  79.33333333 139.33333333 221.66666667 161.33333333 173.33333333\n",
      " 195.66666667 148.33333333 145.         123.33333333  85.\n",
      " 181.33333333 149.33333333 120.         186.33333333  54.\n",
      "  79.66666667  86.33333333 141.66666667 113.33333333 118.\n",
      " 106.33333333  76.66666667  64.         178.33333333 196.33333333\n",
      " 113.33333333 140.         229.66666667  99.66666667 375.\n",
      " 147.         195.         195.         123.         121.\n",
      " 313.33333333 149.66666667 256.66666667 204.33333333 212.33333333\n",
      " 125.          98.66666667 105.         145.         148.33333333\n",
      " 148.33333333 123.33333333 183.         186.33333333 210.\n",
      "  94.33333333 216.66666667 141.         112.66666667 150.\n",
      " 480.         106.         111.33333333  79.33333333  99.66666667\n",
      " 150.          68.33333333 139.33333333 194.66666667 181.33333333\n",
      " 159.66666667 117.66666667 118.66666667 187.66666667  96.33333333\n",
      " 121.         113.33333333 128.33333333 173.33333333 320.33333333\n",
      " 105.          76.66666667  94.         118.33333333  91.66666667\n",
      " 105.         488.33333333 238.33333333 175.33333333 166.33333333\n",
      " 161.66666667 608.         119.66666667 130.66666667 139.\n",
      "  83.33333333 118.66666667 110.66666667 121.66666667 135.\n",
      " 100.66666667 148.33333333  86.66666667 401.33333333 129.\n",
      " 112.66666667 121.33333333  79.33333333 200.         260.\n",
      " 150.         183.33333333 106.66666667  97.33333333  93.33333333\n",
      "  93.         446.33333333 127.33333333  63.          93.\n",
      " 106.66666667  76.66666667 136.66666667 146.66666667 121.66666667\n",
      " 105.         166.33333333 113.         149.33333333 488.33333333\n",
      "  96.33333333 123.         446.33333333 112.33333333  96.33333333\n",
      " 135.66666667 140.33333333 123.          66.         111.33333333\n",
      " 485.          93.33333333 185.66666667  87.33333333  95.\n",
      " 148.33333333  76.66666667 187.66666667 111.33333333 133.33333333\n",
      " 313.33333333  98.66666667 117.33333333 200.         116.\n",
      " 222.66666667  40.         145.         320.33333333  56.33333333\n",
      " 106.33333333 115.         310.          93.         218.66666667\n",
      " 182.33333333 100.          86.33333333 186.         118.33333333\n",
      " 485.          48.33333333  71.66666667  66.66666667 123.33333333\n",
      " 104.66666667 190.         135.66666667 218.33333333 139.66666667\n",
      "  99.         123.33333333  98.66666667 260.         193.33333333\n",
      "  99.33333333 113.         115.33333333 110.         164.33333333\n",
      " 188.33333333 121.          46.33333333 401.33333333 129.\n",
      "  55.          87.66666667 128.33333333 116.         189.66666667]\n"
     ]
    }
   ],
   "source": [
    "X_train1 = X_train[0:1000][numberCols].to_numpy()\n",
    "X_test1 = X_test[0:1000][numberCols].to_numpy()\n",
    "y_train1 = y_train[0:1000].to_numpy()[:,0]\n",
    "y_test1 = y_test[0:1000].to_numpy()[:,0]\n",
    "\n",
    "knnr = KNNRegressor(n_neighbors = 3, metric = 'euclidean', mode = 'uniform')\n",
    "knnr.fit(X_train1, y_train1)\n",
    "res = knnr.predict(X_test1)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2: Категориальные признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.1 (1 балл)</b>\n",
    "Реализуйте три функции расстояния на категориальных признаках, которые обсуждались на [третьем семинаре](https://github.com/mmp-mmro-team/mmp_mmro_fall_2019/blob/master/lecture-notes/Sem03_knn.pdf). Не забудьте, что KNNRegressor должен уметь работать с этими функциями расстояния. Как вариант, можно реализовать метрики как [user-defined distance](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train[0:1000][categoricalCols].to_numpy()\n",
    "X_test1 = X_test[0:1000][categoricalCols].to_numpy()\n",
    "y_train1 = y_train[0:1000].to_numpy()[:,0]\n",
    "y_test1 = y_test[0:1000].to_numpy()[:,0]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "X_train1[:, 0] = labelencoder.fit_transform(X_train1[:, 0])\n",
    "X_train1[:, 1] = labelencoder.fit_transform(X_train1[:, 1])\n",
    "X_train1[:, 2] = labelencoder.fit_transform(X_train1[:, 2])\n",
    "\n",
    "X_test1[:, 0] = labelencoder.fit_transform(X_test1[:, 0])\n",
    "X_test1[:, 1] = labelencoder.fit_transform(X_test1[:, 1])\n",
    "X_test1[:, 2] = labelencoder.fit_transform(X_test1[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.2 (1 балл)</b> Найдите все категориальные признаки в данных. Подсчитайте для каждой из метрик качество на тестовой выборке `X_test` при числе соседей $k = 10$. Качество измеряйте с помощью RMSE.\n",
    "\n",
    "Какая функция расстояния оказалась лучшей? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ваш код здесь\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k_max = 10\n",
    "metrics_array = ['overlap', 'flattened_overlap', 'log_overlap']\n",
    "res = np.zeros(k_max * len(metrics_array)).reshape(k_max, len(metrics_array))\n",
    "for m in range(len(metrics_array)):\n",
    "    for k in range(k_max):\n",
    "        knnr = KNNRegressor(n_neighbors = k + 1, metric = metrics_array[m], mode = 'uniform')\n",
    "        knnr.fit(X_train1, y_train1)\n",
    "        pred = knnr.predict(X_test1)\n",
    "        res[k][m] = np.sqrt(mean_squared_error(y_test1, pred))\n",
    "\n",
    "plt.plot(res)\n",
    "plt.legend(metrics_array, loc='lower right')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.3 (1 балл) бонус</b> Подберите лучшее (на тестовой выборке) число соседей $k$ для каждой из функций расстояния. Какого удалось достичь уровня качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.4 (2.5 балла)</b> Отойдем ненадолго от задачи регрессии и перейдём к задаче классификации: будем определять, являеться ли квартира дорогой $(target = 1)$ или дешевой $(target = 0)$. Будем считать дорогими квариры, цена которых выше среднего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['target'] = (data.price > data.price.mean()).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте счетчики, которые заменят категориальные признаки на вещественные.\n",
    "\n",
    "А именно, для каждого категориального признака $f_j(x)$ необходимо сделать следующее:\n",
    "1. Число `counts` объектов в обучающей выборке с таким же значением признака.\n",
    "\\begin{align}\n",
    "counts_j(c) = \\sum_{i=1}^l [f_j(x_i) = c]\n",
    "\\end{align}\n",
    "2. Число `successes` объектов первого класса ($y = 1$) в обучающей выборке с таким же значением признака.\n",
    "\\begin{align}\n",
    "successes_j(c) = \\sum_{i=1}^l[f_j(x_i) = c][y_i = +1].\n",
    "\\end{align}\n",
    "3. Сглаженное отношение двух предыдущих величин:\n",
    "\\begin{align}\n",
    "p_j(c) = \\frac{successes_j(c) + a}{counts_j(c) + b},\n",
    "\\end{align}\n",
    "\n",
    "где $a$ и $b$ - априорные счетчики (например, a = 1, b = 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def counters(x):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        x: value on categorical feature for N objects\n",
    "    returns: vector of length N\n",
    "    \"\"\"\n",
    "    # Ваш код здесь\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку признаки, содержащие информацию о целевой переменной, могут привести к переобучению, может оказаться полезным сделать *фолдинг*: разбить обучающую выборку на $n$ частей, и для $i$-й части считать `counts` и `successes` по всем остальным частям. Для тестовой выборки используются счетчики, посчитанный по всей обучающей выборке. Реализуйте и такой вариант. Достаточно взять $n = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fold_counters(x):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        x: value on categorical feature for N objects\n",
    "    returns: vector of length N\n",
    "    \"\"\"\n",
    "    # Ваш код здесь\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитайте на тесте AUC-ROC метода $k$ ближайших соседей с евклидовой метрикой для выборки, где категориальные признаки заменены на счетчики. Сравните по AUC-ROC два варианта формирования выборки — с фолдингом и без. Не забудьте подобрать наилучшее число соседей $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.5 (1 балл)</b> Вернемся к задаче регрессии. Утверждается, что для задачи регрессии можно также сделать преобразование категориальных признаков в действительные числа. Для этого достаточно для каждого значения признака $f_j$ вычислить:\n",
    "\\begin{align}\n",
    "p_j(c) = g(T_i | f_j(x_i) = c),\n",
    "\\end{align}\n",
    "\n",
    "где $T_i$ - значения целевой переменной объекта $x_i$. Функция $g$ - среднее (mean) или среднеквадратичное отклонение (std).\n",
    "\n",
    "Закодируйте категориальные признаки обоими способами и найдите значение RMSE. Используйте евклидову метрику для поиска ближайших соседей. Для какой функции $g$ значение RMSE лучше? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3: Текстовые признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3.1 (2 балла)</b> Перейдем от категориальным признаков к текстовым. Рассмотрим 2 способа преобразования текста в действительные числа:\n",
    "- Мешок слов (Bag of Words)\n",
    "- TF-IDF\n",
    "\n",
    "[Здесь](https://scikit-learn.org/stable/modules/feature_extraction.html) вы можете прочитать про их применение в Питоне.\n",
    "\n",
    "Сравните оба способа на задаче регресси. Какую лучше метрику использовать: евклидову или косинусную меру? Постройте графики зависимости качества решения задачи от способа преобразования, метрики и количества соседей. Мера качества - RMSE.\n",
    "\n",
    "Объясните полученные результаты.\n",
    "\n",
    "Перед преобразованием не забудьте уменьшить размер словаря. Например, это можно сделать за счет приведения всех слов к одному регистру и удаления [стопслов](https://en.wikipedia.org/wiki/Stop_words) (артиклей, предлогов, союзов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3.2 (1 балл)</b> Используя все доступные признаки, решите задачу регрессии. Для категориальных и текстовых признаков выберите лучшие преобразования. Повлияло ли добавление количественного признака на метрику качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 4: Выводы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваши выводы здесь (ノ°∀°)ノ⌒･*:.｡. .｡.:*･゜ﾟ･*☆"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
